{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea2b7c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import comet_ml\n",
    "# Replace with your actual Comet API key\n",
    "comet_api_key = \"yd6X45GKLWoIKspb4dChTuwOs\"\n",
    "import torch \n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "import mitdeeplearning as mdl\n",
    "import numpy as np\n",
    "import time , os , functools\n",
    "from IPython import display\n",
    "from tqdm import tqdm\n",
    "from scipy.io.wavfile import write\n",
    "\n",
    "# Check if GPU is available, otherwise use CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00f3c34a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 817 songs in text\n",
      "X:4\n",
      "T:Blackbird\n",
      "Z: id:dc-hornpipe-4\n",
      "M:C|\n",
      "L:1/8\n",
      "K:D Mixolydian\n",
      "AG|F2FA GFD2|de (3fed d^cAF|G2GF GFDE|FdcA G2AG|!\n",
      "F2FA GFD2|de (3fed d^cAG|AdcA GcAG|F2D2 D2:|!\n",
      "fg|agfa gfeg|fd e^c d=cA2|agfa gfde|fdgf e2 fg|!\n",
      "a2ge f3e|d^cde fdAG|AdcA GcAG|F2D2 D2:|!\n"
     ]
    }
   ],
   "source": [
    "songs = mdl.lab1.load_training_data()\n",
    "\n",
    "exa = songs[3]\n",
    "print(exa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25ec904b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl.lab1.play_song(exa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05f9f074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 83 unique characters in the dataset\n"
     ]
    }
   ],
   "source": [
    "songs_joined = \"\\n\\n\".join(songs)\n",
    "\n",
    "# Find all unique characters in the joined string\n",
    "vocab = sorted(set(songs_joined))\n",
    "print(\"There are\", len(vocab), \"unique characters in the dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14d4ed63",
   "metadata": {},
   "outputs": [],
   "source": [
    "char2idx = {u: i for i, u in enumerate(vocab)}\n",
    "\n",
    "idx2char = np.array(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36240ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('{')\n",
    "for char, _ in zip(char2idx, range(20)):\n",
    "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
    "print('  ...\\n}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9cc0c65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Vectorize the songs string ###\n",
    "\n",
    "'''TODO: Write a function to convert the all songs string to a vectorized\n",
    "    (i.e., numeric) representation. Use the appropriate mapping\n",
    "    above to convert from vocab characters to the corresponding indices.\n",
    "\n",
    "  NOTE: the output of the `vectorize_string` function\n",
    "  should be a np.array with `N` elements, where `N` is\n",
    "  the number of characters in the input string\n",
    "'''\n",
    "songs_joined = \"\\n\\n\".join(songs)\n",
    "def vectorize_string(string):\n",
    "  vectorize_string = np.array([char2idx[char] for char in string])\n",
    "  return vectorize_string\n",
    "\n",
    "vectorized_songs = vectorize_string(songs_joined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8c05dd26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'X:1\\nT:Alex' ---- characters mapped to int ----> [49 22 13  0 45 22 26 67 60 79]\n"
     ]
    }
   ],
   "source": [
    "print ('{} ---- characters mapped to int ----> {}'.format(repr(songs_joined[:10]), vectorized_songs[:10]))\n",
    "# check that vectorized_songs is a numpy array\n",
    "assert isinstance(vectorized_songs, np.ndarray), \"returned result should be a numpy array\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4532eb6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch function works correctly!\n"
     ]
    }
   ],
   "source": [
    "### Batch definition to create training examples ###\n",
    "\n",
    "def get_batch(vectorized_songs, seq_length, batch_size):\n",
    "    # the length of the vectorized songs string\n",
    "    n = vectorized_songs.shape[0] - 1\n",
    "    # randomly choose the starting indices for the examples in the training batch\n",
    "    idx = np.random.choice(n - seq_length, batch_size)\n",
    "\n",
    "    '''TODO: construct a list of input sequences for the training batch'''\n",
    "    input_batch = [vectorized_songs[i : i +seq_length] for i in idx]\n",
    "\n",
    "    '''TODO: construct a list of output sequences for the training batch'''\n",
    "    output_batch = [vectorized_songs[i+1 : i + seq_length +1] for i in idx]\n",
    "\n",
    "    # Convert the input and output batches to tensors\n",
    "    x_batch = torch.tensor(input_batch, dtype=torch.long)\n",
    "    y_batch = torch.tensor(output_batch, dtype=torch.long)\n",
    "\n",
    "    return x_batch, y_batch\n",
    "\n",
    "# Perform some simple tests to make sure your batch function is working properly!\n",
    "test_args = (vectorized_songs, 10, 2)\n",
    "x_batch, y_batch = get_batch(*test_args)\n",
    "assert x_batch.shape == (2, 10), \"x_batch shape is incorrect\"\n",
    "assert y_batch.shape == (2, 10), \"y_batch shape is incorrect\"\n",
    "print(\"Batch function works correctly!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc6d014",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_batch, y_batch = get_batch(vectorized_songs, seq_length=5, batch_size=1)\n",
    "\n",
    "for i, (input_idx, target_idx) in enumerate(zip(x_batch[0], y_batch[0])):\n",
    "    print(\"Step {:3d}\".format(i))\n",
    "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx.item()])))\n",
    "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx.item()])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8307b5",
   "metadata": {},
   "source": [
    "Defining the RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "10991a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the model \n",
    "class lstmmodel(nn.Module):\n",
    "    def __init__(self,vocab_size,embedding_dim , hidden_size):\n",
    "        super(lstmmodel ,self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(vocab_size , embedding_dim)\n",
    "\n",
    "        self.lstm = nn.LSTM(embedding_dim , hidden_size , batch_first = True)\n",
    "\n",
    "        self.fc = nn.Linear(hidden_size , vocab_size)\n",
    "\n",
    "    def init_hidden (self,batch_size,device) :\n",
    "        #initialize both hidden state and cell state\n",
    "        return (torch.zeros(1,batch_size,self.hidden_size).to(device),\n",
    "                torch.zeros(1,batch_size,self.hidden_size).to(device))\n",
    "    def forward(self, x , state = None , return_state = False ):\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        if state is None :\n",
    "            state = self.init_hidden(x.size(0) , x.device)\n",
    "        out,state = self.lstm(x , state)\n",
    "\n",
    "        out = self.fc(out)\n",
    "        return out if not return_state else (out,state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9cf2ed9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "lstmmodel(\n",
      "  (embedding): Embedding(83, 256)\n",
      "  (lstm): LSTM(256, 1024, batch_first=True)\n",
      "  (fc): Linear(in_features=1024, out_features=83, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(vocab)\n",
    "embedding_dim = 256 \n",
    "hidden_size = 1024\n",
    "batch_size = 8\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "model = lstmmodel(vocab_size,embedding_dim,hidden_size).to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4766f0ef",
   "metadata": {},
   "source": [
    "#### Testing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "19657e1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape :  torch.Size([32, 100]) \n",
      "prediction shape :   torch.Size([32, 100, 83])\n"
     ]
    }
   ],
   "source": [
    "x , y = get_batch(vectorized_songs , seq_length=100 , batch_size=32)\n",
    "x = x.to(device)\n",
    "y = y.to(device)\n",
    "\n",
    "pred = model(x)\n",
    "print(f\"Input shape :  {x.shape} \")\n",
    "print(f\"prediction shape :   {pred.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d6a629f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5, 81, 67, 23, 27, 56, 76, 68,  6, 30, 37, 35, 80, 45, 42, 34,  6,\n",
       "       55, 32, 24, 25, 79, 42, 56, 68, 63, 75, 24, 12, 41, 13,  7, 20, 46,\n",
       "       33, 34, 19,  8, 58, 15, 18, 57, 15, 66, 42, 78, 11, 17, 29, 65, 59,\n",
       "       11, 19, 72, 28, 26, 34, 30,  5, 70, 52, 23, 44,  3, 29, 30, 30, 55,\n",
       "        0, 46,  4,  4, 34, 12, 25, 14, 29, 67, 41, 17, 48, 62, 15, 66, 77,\n",
       "       77, 52, 43, 71, 69, 76,  8, 12, 30,  3, 15, 48, 80, 11, 53])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices = torch.multinomial(torch.softmax(pred[0], dim=-1), num_samples=1)\n",
    "sampled_indices = sampled_indices.squeeze(-1).cpu().numpy()\n",
    "sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3235d082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: \n",
      " 'Bde d2Bd|e2eg e3g|!\\nG2 (3GAB A2 (3ABc|B2 (3cde d3d|efgf edBd|e2eg e2ga|!\\nbabg agae|gfgd edcB|cBcd ef'\n",
      "\n",
      "Next Char Predictions: \n",
      " '\\'zl<Baum(ELJyTQI(_G=>xQamht=0P1)8UHI7,c36b3kQw/5Djd/7qCAIE\\'o[<S\"DEE_\\nU##I0>2DlP5Wg3kvv[Rpnu,0E\"3Wy/]'\n"
     ]
    }
   ],
   "source": [
    "print(\"Input: \\n\", repr(\"\".join(idx2char[x[0].cpu()])))\n",
    "print()\n",
    "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1469e5",
   "metadata": {},
   "source": [
    "#### Training the model and Loss checking operation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef67d186",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
