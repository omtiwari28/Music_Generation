{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/omtiwari28/Music_Generation/blob/main/Music_generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea2b7c4e",
      "metadata": {
        "id": "ea2b7c4e"
      },
      "outputs": [],
      "source": [
        "!pip install comet_ml > /dev/null 2>&1\n",
        "import comet_ml\n",
        "# Replace with your actual Comet API key\n",
        "comet_api_key = \"yd6X45GKLWoIKspb4dChTuwOs\"\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "\n",
        "!pip install mitdeeplearning --quiet\n",
        "import mitdeeplearning as mdl\n",
        "import numpy as np\n",
        "import time , os , functools\n",
        "from IPython import display\n",
        "from tqdm import tqdm\n",
        "from scipy.io.wavfile import write\n",
        "import pygame\n",
        "!apt-get install abcmidi timidity > /dev/null 2>&1\n",
        "# Check if GPU is available, otherwise use CPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00f3c34a",
      "metadata": {
        "id": "00f3c34a"
      },
      "outputs": [],
      "source": [
        "songs = mdl.lab1.load_training_data()\n",
        "\n",
        "exa = songs[3]\n",
        "print(exa)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25ec904b",
      "metadata": {
        "id": "25ec904b"
      },
      "outputs": [],
      "source": [
        "mdl.lab1.play_song(exa)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05f9f074",
      "metadata": {
        "id": "05f9f074"
      },
      "outputs": [],
      "source": [
        "songs_joined = \"\\n\\n\".join(songs)\n",
        "\n",
        "# Find all unique characters in the joined string\n",
        "vocab = sorted(set(songs_joined))\n",
        "print(\"There are\", len(vocab), \"unique characters in the dataset\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14d4ed63",
      "metadata": {
        "id": "14d4ed63"
      },
      "outputs": [],
      "source": [
        "char2idx = {u: i for i, u in enumerate(vocab)}\n",
        "\n",
        "idx2char = np.array(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e36240ce",
      "metadata": {
        "id": "e36240ce"
      },
      "outputs": [],
      "source": [
        "print('{')\n",
        "for char, _ in zip(char2idx, range(20)):\n",
        "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
        "print('  ...\\n}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9cc0c65b",
      "metadata": {
        "id": "9cc0c65b"
      },
      "outputs": [],
      "source": [
        "### Vectorize the songs string ###\n",
        "\n",
        "'''TODO: Write a function to convert the all songs string to a vectorized\n",
        "    (i.e., numeric) representation. Use the appropriate mapping\n",
        "    above to convert from vocab characters to the corresponding indices.\n",
        "\n",
        "  NOTE: the output of the `vectorize_string` function\n",
        "  should be a np.array with `N` elements, where `N` is\n",
        "  the number of characters in the input string\n",
        "'''\n",
        "songs_joined = \"\\n\\n\".join(songs)\n",
        "def vectorize_string(string):\n",
        "  vectorize_string = np.array([char2idx[char] for char in string])\n",
        "  return vectorize_string\n",
        "\n",
        "vectorized_songs = vectorize_string(songs_joined)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c05dd26",
      "metadata": {
        "id": "8c05dd26"
      },
      "outputs": [],
      "source": [
        "print ('{} ---- characters mapped to int ----> {}'.format(repr(songs_joined[:10]), vectorized_songs[:10]))\n",
        "# check that vectorized_songs is a numpy array\n",
        "assert isinstance(vectorized_songs, np.ndarray), \"returned result should be a numpy array\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4532eb6d",
      "metadata": {
        "id": "4532eb6d"
      },
      "outputs": [],
      "source": [
        "### Batch definition to create training examples ###\n",
        "\n",
        "def get_batch(vectorized_songs, seq_length, batch_size):\n",
        "    # the length of the vectorized songs string\n",
        "    n = vectorized_songs.shape[0] - 1\n",
        "    # randomly choose the starting indices for the examples in the training batch\n",
        "    idx = np.random.choice(n - seq_length, batch_size)\n",
        "\n",
        "    '''TODO: construct a list of input sequences for the training batch'''\n",
        "    input_batch = [vectorized_songs[i : i +seq_length] for i in idx]\n",
        "\n",
        "    '''TODO: construct a list of output sequences for the training batch'''\n",
        "    output_batch = [vectorized_songs[i+1 : i + seq_length +1] for i in idx]\n",
        "\n",
        "    # Convert the input and output batches to tensors\n",
        "    x_batch = torch.tensor(input_batch, dtype=torch.long)\n",
        "    y_batch = torch.tensor(output_batch, dtype=torch.long)\n",
        "\n",
        "    return x_batch, y_batch\n",
        "\n",
        "# Perform some simple tests to make sure your batch function is working properly!\n",
        "test_args = (vectorized_songs, 10, 2)\n",
        "x_batch, y_batch = get_batch(*test_args)\n",
        "assert x_batch.shape == (2, 10), \"x_batch shape is incorrect\"\n",
        "assert y_batch.shape == (2, 10), \"y_batch shape is incorrect\"\n",
        "print(\"Batch function works correctly!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8fc6d014",
      "metadata": {
        "id": "8fc6d014"
      },
      "outputs": [],
      "source": [
        "x_batch, y_batch = get_batch(vectorized_songs, seq_length=5, batch_size=1)\n",
        "\n",
        "for i, (input_idx, target_idx) in enumerate(zip(x_batch[0], y_batch[0])):\n",
        "    print(\"Step {:3d}\".format(i))\n",
        "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx.item()])))\n",
        "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx.item()])))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f8307b5",
      "metadata": {
        "id": "2f8307b5"
      },
      "source": [
        "Defining the RNN Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10991a95",
      "metadata": {
        "id": "10991a95"
      },
      "outputs": [],
      "source": [
        "# defining the model\n",
        "class lstmmodel(nn.Module):\n",
        "    def __init__(self,vocab_size,embedding_dim , hidden_size):\n",
        "        super(lstmmodel ,self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = nn.Embedding(vocab_size , embedding_dim)\n",
        "\n",
        "        self.lstm = nn.LSTM(embedding_dim , hidden_size , batch_first = True)\n",
        "\n",
        "        self.fc = nn.Linear(hidden_size , vocab_size)\n",
        "\n",
        "    def init_hidden (self,batch_size,device) :\n",
        "        #initialize both hidden state and cell state\n",
        "        return (torch.zeros(1,batch_size,self.hidden_size).to(device),\n",
        "                torch.zeros(1,batch_size,self.hidden_size).to(device))\n",
        "    def forward(self, x , state = None , return_state = False ):\n",
        "        x = self.embedding(x)\n",
        "\n",
        "        if state is None :\n",
        "            state = self.init_hidden(x.size(0) , x.device)\n",
        "        out,state = self.lstm(x , state)\n",
        "\n",
        "        out = self.fc(out)\n",
        "        return out if not return_state else (out,state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9cf2ed9d",
      "metadata": {
        "id": "9cf2ed9d"
      },
      "outputs": [],
      "source": [
        "vocab_size = len(vocab)\n",
        "embedding_dim = 256\n",
        "hidden_size = 1024\n",
        "batch_size = 8\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "model = lstmmodel(vocab_size,embedding_dim,hidden_size).to(device)\n",
        "\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4766f0ef",
      "metadata": {
        "id": "4766f0ef"
      },
      "source": [
        "#### Testing the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19657e1a",
      "metadata": {
        "id": "19657e1a"
      },
      "outputs": [],
      "source": [
        "x , y = get_batch(vectorized_songs , seq_length=100 , batch_size=32)\n",
        "x = x.to(device)\n",
        "y = y.to(device)\n",
        "\n",
        "pred = model(x)\n",
        "print(f\"Input shape :  {x.shape} \")\n",
        "print(f\"prediction shape :   {pred.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6a629f6",
      "metadata": {
        "id": "d6a629f6"
      },
      "outputs": [],
      "source": [
        "sampled_indices = torch.multinomial(torch.softmax(pred[0], dim=-1), num_samples=1)\n",
        "sampled_indices = sampled_indices.squeeze(-1).cpu().numpy()\n",
        "sampled_indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3235d082",
      "metadata": {
        "id": "3235d082"
      },
      "outputs": [],
      "source": [
        "print(\"Input: \\n\", repr(\"\".join(idx2char[x[0].cpu()])))\n",
        "print()\n",
        "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices])))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc1469e5",
      "metadata": {
        "id": "cc1469e5"
      },
      "source": [
        "#### Training the model and Loss checking operation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef67d186",
      "metadata": {
        "id": "ef67d186"
      },
      "source": [
        "#### Defining the Lossy Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9dc040f6",
      "metadata": {
        "id": "9dc040f6"
      },
      "outputs": [],
      "source": [
        "### Defining The Lossy Function ###\n",
        "Cross_entropy = nn.CrossEntropyLoss()\n",
        "def compute_loss(labels , logits ):\n",
        "    \"\"\"\n",
        "        Inputs :\n",
        "            labels: (batch_size , sequence_length)\n",
        "            logits: (batch_size , sequence_length , vocab_size)\n",
        "\n",
        "        output :\n",
        "            loss : scalar cross entropy loss over the batch and sequence length\n",
        "        \"\"\"\n",
        "\n",
        "    batched_labels = labels.view(-1)\n",
        "    batched_logits = logits.view(-1 , logits.size(-1))\n",
        "\n",
        "    loss = Cross_entropy(batched_logits , batched_labels)\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1548d795",
      "metadata": {
        "id": "1548d795"
      },
      "outputs": [],
      "source": [
        "### Compute the loss on the prediction from the untrained model from earlier ###\n",
        "y.shape\n",
        "pred.shape\n",
        "\n",
        "example_batch_loss = compute_loss( y, pred)\n",
        "\n",
        "print(f\"Prediction Shape: {pred.shape} #(Batch_size , Sequence_lenght , vocab_size)\")\n",
        "print(f\"Scalar.loss : {example_batch_loss.mean().item()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6d0ce29",
      "metadata": {
        "id": "b6d0ce29"
      },
      "outputs": [],
      "source": [
        "### HyperParameters Setting and Optimization ###\n",
        "Vocal_size = len(vocab)\n",
        "\n",
        "# Model Parameters\n",
        "para = dict(\n",
        "    num_training_iterations = 300,\n",
        "     batch_size = 8,\n",
        "     seq_length = 100,\n",
        "     learning_rate = 5e-3,\n",
        "     embedding_dim = 256,\n",
        "     hidden_size = 1024\n",
        ")\n",
        "\n",
        "#CHeckout Points\n",
        "Checkpoint_dir = './Training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(Checkpoint_dir , \"my_ckpt\")\n",
        "os.makedirs(Checkpoint_dir , exist_ok=True )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18ccc0b5",
      "metadata": {
        "id": "18ccc0b5"
      },
      "outputs": [],
      "source": [
        "### CREATE a Comet Experiment to track Our training Experiment ###\n",
        "\n",
        "def create_experiment ():\n",
        "    # End any Prior Experiments\n",
        "    if 'experiment' in locals():\n",
        "        experiment.end()\n",
        "\n",
        "    experiment = comet_ml.Experiment(\n",
        "        api_key=comet_api_key ,\n",
        "        project_name = \"Genrating_Music\" )\n",
        "\n",
        "    # log our hyperparameters  , defined above , to the parameters\n",
        "    for param , value  in para.items():\n",
        "        experiment.log_parameter(param , value)\n",
        "    experiment.flush()\n",
        "\n",
        "    return experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3d38e69",
      "metadata": {
        "id": "e3d38e69"
      },
      "outputs": [],
      "source": [
        "### Define optimizer and training operation ###\n",
        "model = lstmmodel(vocab_size , embedding_dim , hidden_size)\n",
        "model.to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
        "\n",
        "def train_step(x,y):\n",
        "    model.train()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    y_hat = model(x)\n",
        "\n",
        "    loss = compute_loss(y , y_hat)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e656d81d",
      "metadata": {
        "id": "e656d81d"
      },
      "outputs": [],
      "source": [
        "### TRAINING THE DATA STARTS ###\n",
        "history = []\n",
        "plotter = mdl.util.PeriodicPlotter(sec = 2 , xlabel='iterations', ylabel='loss')\n",
        "experiment = create_experiment()\n",
        "\n",
        "if hasattr(tqdm , '_instances'): tqdm._instances.clear()\n",
        "for iter in tqdm(range(para[\"num_training_iterations\"])):\n",
        "    x_batch , y_batch = get_batch(vectorized_songs , para['seq_length'], para['batch_size'])\n",
        "\n",
        "    x_batch = torch.tensor(x_batch , dtype=torch.long).to(device)\n",
        "    y_batch = torch.tensor(y_batch , dtype=torch.long).to(device)\n",
        "\n",
        "    loss = train_step(x_batch , y_batch)\n",
        "    experiment.log_metric(\"loss\" , loss.item(), step = iter)\n",
        "\n",
        "    history.append(loss.item())\n",
        "    plotter.plot(history)\n",
        "\n",
        "    if iter % 100 == 0:\n",
        "        torch.save(model.state_dict(),checkpoint_prefix)\n",
        "\n",
        "\n",
        "    torch.save(model.state_dict,checkpoint_prefix)\n",
        "    experiment.flush()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f663fbd3",
      "metadata": {
        "id": "f663fbd3"
      },
      "outputs": [],
      "source": [
        "def generate_text(model, start_string, char2idx, idx2char, device, generation_length=1000):\n",
        "    model.eval()\n",
        "    input_idx = [char2idx[char] for char in start_string]\n",
        "    input_idx = torch.tensor([input_idx], dtype=torch.long).to(device)\n",
        "\n",
        "    state = model.init_hidden(input_idx.size(0), device)\n",
        "    text_generated = []\n",
        "\n",
        "    for i in tqdm(range(generation_length)):\n",
        "        predictions, state = model(input_idx, state, return_state=True)\n",
        "        predictions = predictions.squeeze(0)\n",
        "        input_idx = torch.multinomial(torch.softmax(predictions[-1], dim=-1), num_samples=1)\n",
        "        text_generated.append(idx2char[input_idx.item()])\n",
        "        input_idx = input_idx.unsqueeze(0)\n",
        "\n",
        "    return start_string + ''.join(text_generated)\n",
        "\n",
        "generated_text = generate_text(model, \"X:\", char2idx, idx2char, device)\n",
        "#mdl.lab1.play_generated_song(generate_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32c54a59",
      "metadata": {
        "id": "32c54a59"
      },
      "outputs": [],
      "source": [
        "### Play back generated songs ###\n",
        "\n",
        "generated_songs = mdl.lab1.extract_song_snippet(generated_text)\n",
        "\n",
        "for i, song in enumerate(generated_songs):\n",
        "  # Synthesize the waveform from a song\n",
        "  waveform = mdl.lab1.play_song(song)\n",
        "\n",
        "  # If its a valid song (correct syntax), lets play it!\n",
        "  if waveform:\n",
        "    print(\"Generated song\", i)\n",
        "    display.display(waveform)\n",
        "\n",
        "    numeric_data = np.frombuffer(waveform.data, dtype=np.int16)\n",
        "    wav_file_path = f\"output_{i}.wav\"\n",
        "    write(wav_file_path, 88200, numeric_data)\n",
        "\n",
        "    # save your song to the Comet interface -- you can access it there\n",
        "    experiment.log_asset(wav_file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5dab7cf3",
      "metadata": {
        "id": "5dab7cf3"
      },
      "outputs": [],
      "source": [
        "# when done, end the comet experiment\n",
        "experiment.end()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}